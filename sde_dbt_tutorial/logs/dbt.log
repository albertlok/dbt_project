

============================== 2022-08-08 17:07:33.405971 | 3b984096-58bd-4d43-b45f-b0a99bbc3a11 ==============================
[0m17:07:33.406045 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:07:33.406822 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:07:33.407249 [debug] [MainThread]: Tracking: do not track
[0m17:07:33.586423 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m17:07:33.866484 [debug] [MainThread]: Parsing macros/catalog.sql
[0m17:07:33.900544 [debug] [MainThread]: Parsing macros/relations.sql
[0m17:07:33.902445 [debug] [MainThread]: Parsing macros/adapters.sql
[0m17:07:33.935604 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
[0m17:07:33.937624 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:07:33.938385 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:07:33.940015 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:07:33.948048 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:07:33.948740 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:07:33.950118 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:07:33.951342 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m17:07:33.957585 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m17:07:33.960933 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m17:07:33.962940 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m17:07:33.982447 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m17:07:33.998143 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m17:07:34.015300 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m17:07:34.023118 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m17:07:34.025155 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m17:07:34.028348 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m17:07:34.033564 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m17:07:34.053514 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m17:07:34.055257 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m17:07:34.065952 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m17:07:34.086248 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m17:07:34.093159 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m17:07:34.096531 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m17:07:34.102764 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m17:07:34.104644 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m17:07:34.108350 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m17:07:34.111296 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m17:07:34.119802 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m17:07:34.142010 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m17:07:34.144181 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m17:07:34.147308 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m17:07:34.149121 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m17:07:34.150164 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m17:07:34.151139 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m17:07:34.151970 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m17:07:34.153556 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m17:07:34.158719 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m17:07:34.169346 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m17:07:34.170917 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m17:07:34.172351 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m17:07:34.173443 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m17:07:34.174619 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:07:34.176161 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m17:07:34.177361 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m17:07:34.178699 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m17:07:34.179999 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:07:34.182768 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:07:34.184164 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m17:07:34.185477 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m17:07:34.187260 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m17:07:34.188831 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:07:34.189974 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m17:07:34.191250 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m17:07:34.192421 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m17:07:34.199072 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:07:34.200271 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:07:34.203220 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:07:34.205955 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m17:07:34.207367 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m17:07:34.211062 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m17:07:34.214121 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m17:07:34.233171 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m17:07:34.236648 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m17:07:34.252313 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m17:07:34.258639 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m17:07:34.267425 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m17:07:34.279942 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m17:07:34.589780 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__customers.sql
[0m17:07:34.602979 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__orders.sql
[0m17:07:34.605656 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__state.sql
[0m17:07:34.608380 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:07:34.611522 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:07:34.614256 [debug] [MainThread]: 1699: static parser successfully parsed marts/marketing/customer_orders.sql
[0m17:07:34.799959 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:07:34.801907 [info ] [MainThread]: 
[0m17:07:34.802530 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:07:34.803418 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:07:34.814909 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:07:34.815199 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:07:34.815422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:35.002732 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  role "dbt" does not exist

[0m17:07:36.014180 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:07:36.014522 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:07:36.014810 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:07:36.015024 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:07:36.015241 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:07:36.016028 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:07:36.016235 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:10:49.236128 | 89b39042-fafb-4dcd-8beb-79b40bae78ba ==============================
[0m17:10:49.236176 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:10:49.237822 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:10:49.238120 [debug] [MainThread]: Tracking: do not track
[0m17:10:49.364910 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:10:49.365320 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:10:49.429990 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:10:49.432248 [info ] [MainThread]: 
[0m17:10:49.432798 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:10:49.433861 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:10:49.444519 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:10:49.444797 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:10:49.444960 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:10:49.468780 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  role "dbt" does not exist

[0m17:10:50.477882 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:10:50.478245 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:10:50.478542 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:10:50.478748 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:10:50.479000 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:10:50.479851 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:10:50.480062 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:15:25.876787 | 0c590f17-0dd2-424e-a232-c0ea1b6f6f6a ==============================
[0m17:15:25.876843 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:15:25.878148 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:15:25.878398 [debug] [MainThread]: Tracking: do not track
[0m17:15:26.004029 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:15:26.004777 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:15:26.025233 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:15:26.027518 [info ] [MainThread]: 
[0m17:15:26.028078 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:15:26.029015 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:15:26.040526 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:15:26.040754 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:15:26.040915 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:26.194930 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  role "dbt" does not exist

[0m17:15:27.205483 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:15:27.205809 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:15:27.206076 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:15:27.206260 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:15:27.206491 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:15:27.207291 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:15:27.207502 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:15:38.253965 | 93889eec-3b0e-449f-b08a-e413930f6743 ==============================
[0m17:15:38.254000 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:15:38.254777 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:15:38.255240 [debug] [MainThread]: Tracking: do not track
[0m17:15:38.373101 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:15:38.373459 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:15:38.394608 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:15:38.396809 [info ] [MainThread]: 
[0m17:15:38.397432 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:15:38.398530 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:15:38.409466 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:15:38.409730 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:15:38.409897 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:15:38.416787 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  role "dbt" does not exist

[0m17:15:39.422022 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:15:39.422362 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:15:39.422652 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:15:39.422852 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:15:39.423128 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:15:39.424091 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:15:39.424407 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:16:23.641334 | da113e81-dad9-47cd-967e-6287928b1b95 ==============================
[0m17:16:23.641443 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:16:23.642614 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:16:23.642826 [debug] [MainThread]: Tracking: do not track
[0m17:16:23.823485 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:16:23.824030 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:16:23.844877 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:16:23.846980 [info ] [MainThread]: 
[0m17:16:23.847622 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:16:23.849190 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:16:23.859931 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:16:23.860220 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:16:23.860383 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:16:23.886833 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  role "dbt" does not exist

[0m17:16:24.899894 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:16:24.900434 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:16:24.901001 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:16:24.901417 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:16:24.901784 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:16:24.902807 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:16:24.903187 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:24:21.482584 | 8d82a320-835d-4d3e-b24f-b0cb5a72fd90 ==============================
[0m17:24:21.482635 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:24:21.483666 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:24:21.483908 [debug] [MainThread]: Tracking: do not track
[0m17:24:21.535317 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:24:21.740340 [debug] [MainThread]: Parsing macros/catalog.sql
[0m17:24:21.745847 [debug] [MainThread]: Parsing macros/relations.sql
[0m17:24:21.747547 [debug] [MainThread]: Parsing macros/adapters.sql
[0m17:24:21.779671 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
[0m17:24:21.781682 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:24:21.782443 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:24:21.784222 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:24:21.792962 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:24:21.793748 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:24:21.795306 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:24:21.796605 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m17:24:21.801404 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m17:24:21.804216 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m17:24:21.806360 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m17:24:21.826094 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m17:24:21.841601 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m17:24:21.856758 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m17:24:21.862441 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m17:24:21.864800 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m17:24:21.867062 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m17:24:21.872718 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m17:24:21.894001 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m17:24:21.895828 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m17:24:21.907060 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m17:24:21.925438 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m17:24:21.931830 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m17:24:21.935093 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m17:24:21.941887 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m17:24:21.943527 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m17:24:21.947768 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m17:24:21.950708 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m17:24:21.959787 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m17:24:21.982167 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m17:24:21.983987 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m17:24:21.987285 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m17:24:21.989446 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m17:24:21.990511 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m17:24:21.991446 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m17:24:21.992201 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m17:24:21.993735 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m17:24:21.998712 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m17:24:22.009455 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m17:24:22.010815 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m17:24:22.013145 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m17:24:22.014466 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m17:24:22.015623 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:24:22.017354 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m17:24:22.018425 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m17:24:22.019794 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m17:24:22.021096 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:24:22.023789 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:24:22.025237 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m17:24:22.026569 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m17:24:22.027875 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m17:24:22.029373 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:24:22.030538 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m17:24:22.031839 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m17:24:22.033658 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m17:24:22.041401 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:24:22.042620 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:24:22.044868 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:24:22.047265 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m17:24:22.048474 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m17:24:22.051262 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m17:24:22.054448 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m17:24:22.072343 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m17:24:22.075689 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m17:24:22.092539 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m17:24:22.098053 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m17:24:22.107431 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m17:24:22.120646 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m17:24:22.440330 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__customers.sql
[0m17:24:22.454630 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__orders.sql
[0m17:24:22.457538 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__state.sql
[0m17:24:22.460885 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:24:22.463918 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:24:22.466674 [debug] [MainThread]: 1699: static parser successfully parsed marts/marketing/customer_orders.sql
[0m17:24:22.656903 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:24:22.658725 [info ] [MainThread]: 
[0m17:24:22.659331 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:24:22.660194 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:24:22.671346 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:24:22.671668 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:24:22.671868 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:24:22.762609 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  database "dbt" does not exist

[0m17:24:23.773194 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:24:23.773717 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:24:23.774039 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:24:23.774249 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:24:23.774495 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:24:23.775375 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:24:23.775610 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:44:46.191084 | d7febfa0-bb1a-47b3-8c40-0f37fc4632a3 ==============================
[0m17:44:46.191138 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:44:46.192739 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:44:46.193468 [debug] [MainThread]: Tracking: do not track
[0m17:44:46.388018 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:44:46.881389 [debug] [MainThread]: Parsing macros/catalog.sql
[0m17:44:46.886222 [debug] [MainThread]: Parsing macros/relations.sql
[0m17:44:46.887967 [debug] [MainThread]: Parsing macros/adapters.sql
[0m17:44:46.920926 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
[0m17:44:46.922911 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:44:46.923850 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:44:46.926210 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:44:46.933686 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:44:46.934365 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:44:46.936000 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:44:46.937257 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m17:44:46.942471 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m17:44:46.945272 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m17:44:46.947145 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m17:44:46.968399 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m17:44:46.985289 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m17:44:47.000654 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m17:44:47.008475 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m17:44:47.012334 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m17:44:47.014817 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m17:44:47.020295 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m17:44:47.042446 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m17:44:47.045603 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m17:44:47.057221 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m17:44:47.078454 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m17:44:47.085219 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m17:44:47.088915 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m17:44:47.096932 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m17:44:47.098523 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m17:44:47.102710 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m17:44:47.105244 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m17:44:47.115680 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m17:44:47.138373 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m17:44:47.140893 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m17:44:47.144217 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m17:44:47.146054 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m17:44:47.147171 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m17:44:47.148444 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m17:44:47.149400 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m17:44:47.151043 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m17:44:47.156608 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m17:44:47.168195 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m17:44:47.169272 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m17:44:47.170685 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m17:44:47.172241 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m17:44:47.173560 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m17:44:47.175568 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m17:44:47.176662 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m17:44:47.177922 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m17:44:47.179307 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m17:44:47.182383 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m17:44:47.183799 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m17:44:47.185027 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m17:44:47.186410 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m17:44:47.187793 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m17:44:47.188865 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m17:44:47.190234 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m17:44:47.192114 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m17:44:47.199514 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m17:44:47.200695 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m17:44:47.203075 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m17:44:47.205452 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m17:44:47.206749 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m17:44:47.210065 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m17:44:47.213522 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m17:44:47.232187 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m17:44:47.235498 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m17:44:47.251249 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m17:44:47.256387 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m17:44:47.264895 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m17:44:47.276100 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m17:44:47.582521 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__customers.sql
[0m17:44:47.595700 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__orders.sql
[0m17:44:47.598552 [debug] [MainThread]: 1699: static parser successfully parsed staging/stg_eltool__state.sql
[0m17:44:47.601564 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/dim_customers.sql
[0m17:44:47.604259 [debug] [MainThread]: 1699: static parser successfully parsed marts/core/fct_orders.sql
[0m17:44:47.607328 [debug] [MainThread]: 1699: static parser successfully parsed marts/marketing/customer_orders.sql
[0m17:44:47.793988 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:44:47.795822 [info ] [MainThread]: 
[0m17:44:47.796348 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:44:47.797107 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:44:47.809013 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:44:47.809328 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:44:47.809682 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:44:48.200984 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  database "dbt" does not exist

[0m17:44:49.211756 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:44:49.212097 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:44:49.212467 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:44:49.212679 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:44:49.212928 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:44:49.213812 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:44:49.214021 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:55:53.428317 | 036a8327-f00f-4519-b826-ccb1510fa526 ==============================
[0m17:55:53.428348 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:55:53.429946 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:55:53.430292 [debug] [MainThread]: Tracking: do not track
[0m17:55:53.759314 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:55:53.759805 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:55:53.780995 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:55:53.783094 [info ] [MainThread]: 
[0m17:55:53.783632 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:55:53.784537 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:55:53.795236 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:55:53.795518 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:55:53.795704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:55:53.893265 [debug] [ThreadPool]: Postgres adapter: Got a retryable error when attempting to open a postgres connection.
1 attempts remaining. Retrying in 1 seconds.
Error:
connection to server at "localhost" (::1), port 5432 failed: FATAL:  database "dbt" does not exist

[0m17:55:54.901708 [debug] [ThreadPool]: Postgres adapter: Error running SQL: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:55:54.902051 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:55:54.902339 [debug] [ThreadPool]: Postgres adapter: Error running SQL: macro list_schemas
[0m17:55:54.902543 [debug] [ThreadPool]: Postgres adapter: Rolling back transaction.
[0m17:55:54.902786 [debug] [ThreadPool]: On list_dbt: No close available on handle
[0m17:55:54.903639 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:55:54.903821 [debug] [MainThread]: Connection 'list_dbt' was properly closed.


============================== 2022-08-08 17:56:47.575607 | 4d28282e-cc63-42b1-9f18-eca4f7ad18c3 ==============================
[0m17:56:47.575640 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:56:47.577586 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:56:47.577905 [debug] [MainThread]: Tracking: do not track
[0m17:56:47.781169 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:56:47.781658 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:56:47.804560 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:56:47.806730 [info ] [MainThread]: 
[0m17:56:47.807312 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:56:47.808240 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:56:47.819140 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:56:47.819435 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:56:47.819605 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:56:47.916042 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.1 seconds
[0m17:56:47.917883 [debug] [ThreadPool]: On list_dbt: Close
[0m17:56:47.919662 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_snapshots"
[0m17:56:47.926401 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m17:56:47.926692 [debug] [ThreadPool]: On list_dbt_snapshots: BEGIN
[0m17:56:47.926897 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:56:47.940893 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m17:56:47.941252 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m17:56:47.941438 [debug] [ThreadPool]: On list_dbt_snapshots: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_snapshots"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'snapshots'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'snapshots'
  
[0m17:56:47.948563 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.01 seconds
[0m17:56:47.950352 [debug] [ThreadPool]: On list_dbt_snapshots: ROLLBACK
[0m17:56:47.952614 [debug] [ThreadPool]: On list_dbt_snapshots: Close
[0m17:56:47.954021 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_warehouse"
[0m17:56:47.956869 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m17:56:47.957379 [debug] [ThreadPool]: On list_dbt_warehouse: BEGIN
[0m17:56:47.957749 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:56:47.968056 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m17:56:47.968467 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m17:56:47.968771 [debug] [ThreadPool]: On list_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_warehouse"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m17:56:47.973391 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.0 seconds
[0m17:56:47.974824 [debug] [ThreadPool]: On list_dbt_warehouse: ROLLBACK
[0m17:56:47.976013 [debug] [ThreadPool]: On list_dbt_warehouse: Close
[0m17:56:47.981594 [debug] [MainThread]: Using postgres connection "master"
[0m17:56:47.981862 [debug] [MainThread]: On master: BEGIN
[0m17:56:47.982028 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:56:47.993600 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m17:56:47.993887 [debug] [MainThread]: Using postgres connection "master"
[0m17:56:47.994183 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m17:56:48.007258 [debug] [MainThread]: SQL status: SELECT 0 in 0.01 seconds
[0m17:56:48.008647 [debug] [MainThread]: On master: ROLLBACK
[0m17:56:48.010081 [debug] [MainThread]: Using postgres connection "master"
[0m17:56:48.010329 [debug] [MainThread]: On master: BEGIN
[0m17:56:48.012957 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m17:56:48.013256 [debug] [MainThread]: On master: COMMIT
[0m17:56:48.013454 [debug] [MainThread]: Using postgres connection "master"
[0m17:56:48.013842 [debug] [MainThread]: On master: COMMIT
[0m17:56:48.014995 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m17:56:48.015195 [debug] [MainThread]: On master: Close
[0m17:56:48.015622 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:56:48.015982 [info ] [MainThread]: 
[0m17:56:48.025871 [debug] [Thread-1  ]: Began running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:56:48.026238 [info ] [Thread-1  ]: 1 of 1 START snapshot snapshots.customers_snapshot ............................. [RUN]
[0m17:56:48.026754 [debug] [Thread-1  ]: Acquiring new postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:56:48.026951 [debug] [Thread-1  ]: Began compiling node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:56:48.027148 [debug] [Thread-1  ]: Compiling snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:56:48.031399 [debug] [Thread-1  ]: finished collecting timing info
[0m17:56:48.031739 [debug] [Thread-1  ]: Began executing node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:56:48.094826 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:56:48.095758 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:56:48.095944 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: BEGIN
[0m17:56:48.096103 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:56:48.107972 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m17:56:48.108345 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:56:48.108545 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      

  create  table "dbt"."snapshots"."customers_snapshot"
  as (
    

    select *,
        md5(coalesce(cast(customer_id as varchar ), '')
         || '|' || coalesce(cast(datetime_updated as varchar ), '')
        ) as dbt_scd_id,
        datetime_updated as dbt_updated_at,
        datetime_updated as dbt_valid_from,
        nullif(datetime_updated, datetime_updated) as dbt_valid_to
    from (
        



select * from "dbt"."warehouse"."customers"

    ) sbq



  );
  
[0m17:56:48.133316 [debug] [Thread-1  ]: SQL status: SELECT 100 in 0.02 seconds
[0m17:56:48.161193 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: COMMIT
[0m17:56:48.161496 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:56:48.161666 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: COMMIT
[0m17:56:48.166486 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m17:56:48.167130 [debug] [Thread-1  ]: finished collecting timing info
[0m17:56:48.167485 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: Close
[0m17:56:48.168416 [info ] [Thread-1  ]: 1 of 1 OK snapshotted snapshots.customers_snapshot ............................. [[32mSELECT 100[0m in 0.14s]
[0m17:56:48.169367 [debug] [Thread-1  ]: Finished running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:56:48.171101 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:56:48.171384 [debug] [MainThread]: Using postgres connection "master"
[0m17:56:48.171584 [debug] [MainThread]: On master: BEGIN
[0m17:56:48.171779 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:56:48.180385 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m17:56:48.180708 [debug] [MainThread]: On master: COMMIT
[0m17:56:48.180893 [debug] [MainThread]: Using postgres connection "master"
[0m17:56:48.181061 [debug] [MainThread]: On master: COMMIT
[0m17:56:48.182388 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m17:56:48.182733 [debug] [MainThread]: On master: Close
[0m17:56:48.183249 [info ] [MainThread]: 
[0m17:56:48.183725 [info ] [MainThread]: Finished running 1 snapshot in 0 hours 0 minutes and 0.38 seconds (0.38s).
[0m17:56:48.184266 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:56:48.184564 [debug] [MainThread]: Connection 'snapshot.sde_dbt_tutorial.customers_snapshot' was properly closed.
[0m17:56:48.193734 [info ] [MainThread]: 
[0m17:56:48.194163 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:56:48.194495 [info ] [MainThread]: 
[0m17:56:48.194756 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1


============================== 2022-08-08 17:58:01.620931 | 27109c2c-40b0-4589-8f38-bfc16dbda38c ==============================
[0m17:58:01.620965 [info ] [MainThread]: Running with dbt=1.2.0
[0m17:58:01.622545 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m17:58:01.622765 [debug] [MainThread]: Tracking: do not track
[0m17:58:01.984804 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:58:01.985228 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:58:02.006867 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m17:58:02.008866 [info ] [MainThread]: 
[0m17:58:02.009392 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:58:02.010368 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m17:58:02.022538 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m17:58:02.022846 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m17:58:02.023044 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:58:02.053751 [debug] [ThreadPool]: SQL status: SELECT 6 in 0.03 seconds
[0m17:58:02.055464 [debug] [ThreadPool]: On list_dbt: Close
[0m17:58:02.056986 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_snapshots"
[0m17:58:02.064364 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m17:58:02.064679 [debug] [ThreadPool]: On list_dbt_snapshots: BEGIN
[0m17:58:02.064907 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:58:02.074214 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m17:58:02.074474 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m17:58:02.074689 [debug] [ThreadPool]: On list_dbt_snapshots: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_snapshots"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'snapshots'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'snapshots'
  
[0m17:58:02.079583 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.0 seconds
[0m17:58:02.080985 [debug] [ThreadPool]: On list_dbt_snapshots: ROLLBACK
[0m17:58:02.081967 [debug] [ThreadPool]: On list_dbt_snapshots: Close
[0m17:58:02.082615 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_warehouse"
[0m17:58:02.085156 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m17:58:02.085428 [debug] [ThreadPool]: On list_dbt_warehouse: BEGIN
[0m17:58:02.085661 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m17:58:02.095593 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m17:58:02.096029 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m17:58:02.096246 [debug] [ThreadPool]: On list_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_warehouse"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m17:58:02.100082 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.0 seconds
[0m17:58:02.101453 [debug] [ThreadPool]: On list_dbt_warehouse: ROLLBACK
[0m17:58:02.102584 [debug] [ThreadPool]: On list_dbt_warehouse: Close
[0m17:58:02.108207 [debug] [MainThread]: Using postgres connection "master"
[0m17:58:02.108487 [debug] [MainThread]: On master: BEGIN
[0m17:58:02.108667 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:58:02.118827 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m17:58:02.119172 [debug] [MainThread]: Using postgres connection "master"
[0m17:58:02.119381 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m17:58:02.132740 [debug] [MainThread]: SQL status: SELECT 0 in 0.01 seconds
[0m17:58:02.134316 [debug] [MainThread]: On master: ROLLBACK
[0m17:58:02.136248 [debug] [MainThread]: Using postgres connection "master"
[0m17:58:02.136712 [debug] [MainThread]: On master: BEGIN
[0m17:58:02.139677 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m17:58:02.140085 [debug] [MainThread]: On master: COMMIT
[0m17:58:02.140422 [debug] [MainThread]: Using postgres connection "master"
[0m17:58:02.140803 [debug] [MainThread]: On master: COMMIT
[0m17:58:02.142363 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m17:58:02.142589 [debug] [MainThread]: On master: Close
[0m17:58:02.143057 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m17:58:02.143373 [info ] [MainThread]: 
[0m17:58:02.151649 [debug] [Thread-1  ]: Began running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:58:02.152007 [info ] [Thread-1  ]: 1 of 1 START snapshot snapshots.customers_snapshot ............................. [RUN]
[0m17:58:02.152495 [debug] [Thread-1  ]: Acquiring new postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.152697 [debug] [Thread-1  ]: Began compiling node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:58:02.152930 [debug] [Thread-1  ]: Compiling snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:58:02.156702 [debug] [Thread-1  ]: finished collecting timing info
[0m17:58:02.156921 [debug] [Thread-1  ]: Began executing node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:58:02.212978 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.213307 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: BEGIN
[0m17:58:02.213492 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m17:58:02.223165 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m17:58:02.223521 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.223704 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "dbt".INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot'
        
        and table_schema = 'snapshots'
        
      order by ordinal_position

  
[0m17:58:02.231959 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.01 seconds
[0m17:58:02.261655 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.262020 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

        

  create temporary table "customers_snapshot__dbt_tmp105802249352"
  as (
    with snapshot_query as (

        



select * from "dbt"."warehouse"."customers"


    ),

    snapshotted_data as (

        select *,
            customer_id as dbt_unique_key

        from "dbt"."snapshots"."customers_snapshot"
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            customer_id as dbt_unique_key,
            datetime_updated as dbt_updated_at,
            datetime_updated as dbt_valid_from,
            nullif(datetime_updated, datetime_updated) as dbt_valid_to,
            md5(coalesce(cast(customer_id as varchar ), '')
         || '|' || coalesce(cast(datetime_updated as varchar ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            customer_id as dbt_unique_key,
            datetime_updated as dbt_updated_at,
            datetime_updated as dbt_valid_from,
            datetime_updated as dbt_valid_to

        from snapshot_query
    ),

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.dbt_valid_from < source_data.datetime_updated)
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.dbt_valid_from < source_data.datetime_updated)
        )
    )

    select * from insertions
    union all
    select * from updates

  );
    
[0m17:58:02.268588 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.01 seconds
[0m17:58:02.272179 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.272455 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot__dbt_tmp105802249352'
        
      order by ordinal_position

  
[0m17:58:02.276750 [debug] [Thread-1  ]: SQL status: SELECT 12 in 0.0 seconds
[0m17:58:02.280246 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.280440 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "dbt".INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot'
        
        and table_schema = 'snapshots'
        
      order by ordinal_position

  
[0m17:58:02.283872 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.0 seconds
[0m17:58:02.287248 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.287483 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot__dbt_tmp105802249352'
        
      order by ordinal_position

  
[0m17:58:02.292023 [debug] [Thread-1  ]: SQL status: SELECT 12 in 0.0 seconds
[0m17:58:02.295666 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.295898 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "dbt".INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot'
        
        and table_schema = 'snapshots'
        
      order by ordinal_position

  
[0m17:58:02.300511 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.0 seconds
[0m17:58:02.308641 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.308941 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot__dbt_tmp105802249352'
        
      order by ordinal_position

  
[0m17:58:02.313234 [debug] [Thread-1  ]: SQL status: SELECT 12 in 0.0 seconds
[0m17:58:02.320904 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.322750 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.322938 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      update "dbt"."snapshots"."customers_snapshot"
    set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to
    from "customers_snapshot__dbt_tmp105802249352" as DBT_INTERNAL_SOURCE
    where DBT_INTERNAL_SOURCE.dbt_scd_id::text = "dbt"."snapshots"."customers_snapshot".dbt_scd_id::text
      and DBT_INTERNAL_SOURCE.dbt_change_type::text in ('update'::text, 'delete'::text)
      and "dbt"."snapshots"."customers_snapshot".dbt_valid_to is null;

    insert into "dbt"."snapshots"."customers_snapshot" ("customer_id", "zipcode", "city", "state_code", "datetime_created", "datetime_updated", "dbt_updated_at", "dbt_valid_from", "dbt_valid_to", "dbt_scd_id")
    select DBT_INTERNAL_SOURCE."customer_id",DBT_INTERNAL_SOURCE."zipcode",DBT_INTERNAL_SOURCE."city",DBT_INTERNAL_SOURCE."state_code",DBT_INTERNAL_SOURCE."datetime_created",DBT_INTERNAL_SOURCE."datetime_updated",DBT_INTERNAL_SOURCE."dbt_updated_at",DBT_INTERNAL_SOURCE."dbt_valid_from",DBT_INTERNAL_SOURCE."dbt_valid_to",DBT_INTERNAL_SOURCE."dbt_scd_id"
    from "customers_snapshot__dbt_tmp105802249352" as DBT_INTERNAL_SOURCE
    where DBT_INTERNAL_SOURCE.dbt_change_type::text = 'insert'::text;

  
[0m17:58:02.324958 [debug] [Thread-1  ]: SQL status: INSERT 0 5 in 0.0 seconds
[0m17:58:02.343811 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: COMMIT
[0m17:58:02.344124 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m17:58:02.344306 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: COMMIT
[0m17:58:02.346659 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m17:58:02.349641 [debug] [Thread-1  ]: finished collecting timing info
[0m17:58:02.349874 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: Close
[0m17:58:02.350487 [info ] [Thread-1  ]: 1 of 1 OK snapshotted snapshots.customers_snapshot ............................. [[32mINSERT 0 5[0m in 0.20s]
[0m17:58:02.350965 [debug] [Thread-1  ]: Finished running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m17:58:02.352354 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m17:58:02.352718 [debug] [MainThread]: Using postgres connection "master"
[0m17:58:02.353014 [debug] [MainThread]: On master: BEGIN
[0m17:58:02.353261 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m17:58:02.361514 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m17:58:02.361789 [debug] [MainThread]: On master: COMMIT
[0m17:58:02.362008 [debug] [MainThread]: Using postgres connection "master"
[0m17:58:02.362175 [debug] [MainThread]: On master: COMMIT
[0m17:58:02.363053 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m17:58:02.363371 [debug] [MainThread]: On master: Close
[0m17:58:02.363924 [info ] [MainThread]: 
[0m17:58:02.364221 [info ] [MainThread]: Finished running 1 snapshot in 0 hours 0 minutes and 0.35 seconds (0.35s).
[0m17:58:02.364542 [debug] [MainThread]: Connection 'master' was properly closed.
[0m17:58:02.364696 [debug] [MainThread]: Connection 'snapshot.sde_dbt_tutorial.customers_snapshot' was properly closed.
[0m17:58:02.375416 [info ] [MainThread]: 
[0m17:58:02.375915 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:58:02.376257 [info ] [MainThread]: 
[0m17:58:02.376520 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1


============================== 2022-08-08 18:06:50.422924 | 71d1b4f3-e5b4-4a46-8eb6-7e2f3c849e64 ==============================
[0m18:06:50.422969 [info ] [MainThread]: Running with dbt=1.2.0
[0m18:06:50.426799 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'snapshot', 'rpc_method': 'snapshot', 'indirect_selection': 'eager'}
[0m18:06:50.427027 [debug] [MainThread]: Tracking: do not track
[0m18:06:50.864618 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:06:50.865152 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:06:50.905762 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:06:50.908223 [info ] [MainThread]: 
[0m18:06:50.909239 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:06:50.910911 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m18:06:50.923752 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m18:06:50.924082 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m18:06:50.924320 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:06:51.052199 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.13 seconds
[0m18:06:51.054362 [debug] [ThreadPool]: On list_dbt: Close
[0m18:06:51.056638 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_warehouse"
[0m18:06:51.063621 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:06:51.063888 [debug] [ThreadPool]: On list_dbt_warehouse: BEGIN
[0m18:06:51.064073 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:06:51.076261 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m18:06:51.076588 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:06:51.076773 [debug] [ThreadPool]: On list_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_warehouse"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m18:06:51.081652 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.0 seconds
[0m18:06:51.083075 [debug] [ThreadPool]: On list_dbt_warehouse: ROLLBACK
[0m18:06:51.084679 [debug] [ThreadPool]: On list_dbt_warehouse: Close
[0m18:06:51.085518 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_snapshots"
[0m18:06:51.088915 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:06:51.089292 [debug] [ThreadPool]: On list_dbt_snapshots: BEGIN
[0m18:06:51.089475 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:06:51.108011 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m18:06:51.108387 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:06:51.108617 [debug] [ThreadPool]: On list_dbt_snapshots: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_snapshots"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'snapshots'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'snapshots'
  
[0m18:06:51.113227 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.0 seconds
[0m18:06:51.114649 [debug] [ThreadPool]: On list_dbt_snapshots: ROLLBACK
[0m18:06:51.115802 [debug] [ThreadPool]: On list_dbt_snapshots: Close
[0m18:06:51.121562 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:51.121789 [debug] [MainThread]: On master: BEGIN
[0m18:06:51.121960 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:06:51.132717 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:06:51.133129 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:51.133620 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m18:06:51.148912 [debug] [MainThread]: SQL status: SELECT 0 in 0.01 seconds
[0m18:06:51.150241 [debug] [MainThread]: On master: ROLLBACK
[0m18:06:51.151470 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:51.151728 [debug] [MainThread]: On master: BEGIN
[0m18:06:51.155147 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m18:06:51.155769 [debug] [MainThread]: On master: COMMIT
[0m18:06:51.156182 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:51.156506 [debug] [MainThread]: On master: COMMIT
[0m18:06:51.158896 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m18:06:51.159293 [debug] [MainThread]: On master: Close
[0m18:06:51.160062 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:06:51.160398 [info ] [MainThread]: 
[0m18:06:51.167530 [debug] [Thread-1  ]: Began running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:06:51.167941 [info ] [Thread-1  ]: 1 of 1 START snapshot snapshots.customers_snapshot ............................. [RUN]
[0m18:06:51.168564 [debug] [Thread-1  ]: Acquiring new postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.168969 [debug] [Thread-1  ]: Began compiling node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:06:51.169280 [debug] [Thread-1  ]: Compiling snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:06:51.173154 [debug] [Thread-1  ]: finished collecting timing info
[0m18:06:51.173430 [debug] [Thread-1  ]: Began executing node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:06:51.234353 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.234695 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: BEGIN
[0m18:06:51.235031 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:06:51.245659 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:06:51.246194 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.246420 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "dbt".INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot'
        
        and table_schema = 'snapshots'
        
      order by ordinal_position

  
[0m18:06:51.257220 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.01 seconds
[0m18:06:51.291608 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.292026 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

        

  create temporary table "customers_snapshot__dbt_tmp110651278606"
  as (
    with snapshot_query as (

        



select * from "dbt"."warehouse"."customers"


    ),

    snapshotted_data as (

        select *,
            customer_id as dbt_unique_key

        from "dbt"."snapshots"."customers_snapshot"
        where dbt_valid_to is null

    ),

    insertions_source_data as (

        select
            *,
            customer_id as dbt_unique_key,
            datetime_updated as dbt_updated_at,
            datetime_updated as dbt_valid_from,
            nullif(datetime_updated, datetime_updated) as dbt_valid_to,
            md5(coalesce(cast(customer_id as varchar ), '')
         || '|' || coalesce(cast(datetime_updated as varchar ), '')
        ) as dbt_scd_id

        from snapshot_query
    ),

    updates_source_data as (

        select
            *,
            customer_id as dbt_unique_key,
            datetime_updated as dbt_updated_at,
            datetime_updated as dbt_valid_from,
            datetime_updated as dbt_valid_to

        from snapshot_query
    ),

    insertions as (

        select
            'insert' as dbt_change_type,
            source_data.*

        from insertions_source_data as source_data
        left outer join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where snapshotted_data.dbt_unique_key is null
           or (
                snapshotted_data.dbt_unique_key is not null
            and (
                (snapshotted_data.dbt_valid_from < source_data.datetime_updated)
            )
        )

    ),

    updates as (

        select
            'update' as dbt_change_type,
            source_data.*,
            snapshotted_data.dbt_scd_id

        from updates_source_data as source_data
        join snapshotted_data on snapshotted_data.dbt_unique_key = source_data.dbt_unique_key
        where (
            (snapshotted_data.dbt_valid_from < source_data.datetime_updated)
        )
    )

    select * from insertions
    union all
    select * from updates

  );
    
[0m18:06:51.299337 [debug] [Thread-1  ]: SQL status: SELECT 0 in 0.01 seconds
[0m18:06:51.302812 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.303173 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot__dbt_tmp110651278606'
        
      order by ordinal_position

  
[0m18:06:51.311534 [debug] [Thread-1  ]: SQL status: SELECT 12 in 0.01 seconds
[0m18:06:51.315612 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.315972 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "dbt".INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot'
        
        and table_schema = 'snapshots'
        
      order by ordinal_position

  
[0m18:06:51.321871 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.01 seconds
[0m18:06:51.326250 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.326547 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot__dbt_tmp110651278606'
        
      order by ordinal_position

  
[0m18:06:51.331879 [debug] [Thread-1  ]: SQL status: SELECT 12 in 0.01 seconds
[0m18:06:51.335370 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.335573 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from "dbt".INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot'
        
        and table_schema = 'snapshots'
        
      order by ordinal_position

  
[0m18:06:51.340444 [debug] [Thread-1  ]: SQL status: SELECT 10 in 0.0 seconds
[0m18:06:51.348482 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.348773 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      select
          column_name,
          data_type,
          character_maximum_length,
          numeric_precision,
          numeric_scale

      from INFORMATION_SCHEMA.columns
      where table_name = 'customers_snapshot__dbt_tmp110651278606'
        
      order by ordinal_position

  
[0m18:06:51.353449 [debug] [Thread-1  ]: SQL status: SELECT 12 in 0.0 seconds
[0m18:06:51.361299 [debug] [Thread-1  ]: Writing runtime SQL for node "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.363035 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.363239 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "snapshot.sde_dbt_tutorial.customers_snapshot"} */

      update "dbt"."snapshots"."customers_snapshot"
    set dbt_valid_to = DBT_INTERNAL_SOURCE.dbt_valid_to
    from "customers_snapshot__dbt_tmp110651278606" as DBT_INTERNAL_SOURCE
    where DBT_INTERNAL_SOURCE.dbt_scd_id::text = "dbt"."snapshots"."customers_snapshot".dbt_scd_id::text
      and DBT_INTERNAL_SOURCE.dbt_change_type::text in ('update'::text, 'delete'::text)
      and "dbt"."snapshots"."customers_snapshot".dbt_valid_to is null;

    insert into "dbt"."snapshots"."customers_snapshot" ("customer_id", "zipcode", "city", "state_code", "datetime_created", "datetime_updated", "dbt_updated_at", "dbt_valid_from", "dbt_valid_to", "dbt_scd_id")
    select DBT_INTERNAL_SOURCE."customer_id",DBT_INTERNAL_SOURCE."zipcode",DBT_INTERNAL_SOURCE."city",DBT_INTERNAL_SOURCE."state_code",DBT_INTERNAL_SOURCE."datetime_created",DBT_INTERNAL_SOURCE."datetime_updated",DBT_INTERNAL_SOURCE."dbt_updated_at",DBT_INTERNAL_SOURCE."dbt_valid_from",DBT_INTERNAL_SOURCE."dbt_valid_to",DBT_INTERNAL_SOURCE."dbt_scd_id"
    from "customers_snapshot__dbt_tmp110651278606" as DBT_INTERNAL_SOURCE
    where DBT_INTERNAL_SOURCE.dbt_change_type::text = 'insert'::text;

  
[0m18:06:51.365837 [debug] [Thread-1  ]: SQL status: INSERT 0 0 in 0.0 seconds
[0m18:06:51.388643 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: COMMIT
[0m18:06:51.388913 [debug] [Thread-1  ]: Using postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:06:51.389080 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: COMMIT
[0m18:06:51.398030 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m18:06:51.400878 [debug] [Thread-1  ]: finished collecting timing info
[0m18:06:51.401121 [debug] [Thread-1  ]: On snapshot.sde_dbt_tutorial.customers_snapshot: Close
[0m18:06:51.401938 [info ] [Thread-1  ]: 1 of 1 OK snapshotted snapshots.customers_snapshot ............................. [[32mINSERT 0 0[0m in 0.23s]
[0m18:06:51.402509 [debug] [Thread-1  ]: Finished running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:06:51.403894 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:06:51.404181 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:51.404412 [debug] [MainThread]: On master: BEGIN
[0m18:06:51.404630 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:06:51.414762 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:06:51.415100 [debug] [MainThread]: On master: COMMIT
[0m18:06:51.415451 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:51.415812 [debug] [MainThread]: On master: COMMIT
[0m18:06:51.417470 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m18:06:51.417714 [debug] [MainThread]: On master: Close
[0m18:06:51.418189 [info ] [MainThread]: 
[0m18:06:51.418470 [info ] [MainThread]: Finished running 1 snapshot in 0 hours 0 minutes and 0.51 seconds (0.51s).
[0m18:06:51.418784 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:06:51.418953 [debug] [MainThread]: Connection 'snapshot.sde_dbt_tutorial.customers_snapshot' was properly closed.
[0m18:06:51.488553 [info ] [MainThread]: 
[0m18:06:51.489150 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:06:51.489599 [info ] [MainThread]: 
[0m18:06:51.489905 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1


============================== 2022-08-08 18:06:59.459059 | 1ed4ae0c-d24f-4eb8-a123-0527c8de53c0 ==============================
[0m18:06:59.459093 [info ] [MainThread]: Running with dbt=1.2.0
[0m18:06:59.460531 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:06:59.460748 [debug] [MainThread]: Tracking: do not track
[0m18:06:59.753234 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:06:59.753934 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:06:59.775297 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:06:59.777518 [info ] [MainThread]: 
[0m18:06:59.778181 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:06:59.779389 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt"
[0m18:06:59.789717 [debug] [ThreadPool]: Using postgres connection "list_dbt"
[0m18:06:59.790025 [debug] [ThreadPool]: On list_dbt: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt"} */

    select distinct nspname from pg_namespace
  
[0m18:06:59.790188 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:06:59.805761 [debug] [ThreadPool]: SQL status: SELECT 8 in 0.02 seconds
[0m18:06:59.807632 [debug] [ThreadPool]: On list_dbt: Close
[0m18:06:59.809376 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_warehouse"
[0m18:06:59.819589 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:06:59.819853 [debug] [ThreadPool]: On list_dbt_warehouse: BEGIN
[0m18:06:59.820184 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:06:59.834490 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m18:06:59.834984 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:06:59.835256 [debug] [ThreadPool]: On list_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_warehouse"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m18:06:59.852496 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.02 seconds
[0m18:06:59.855133 [debug] [ThreadPool]: On list_dbt_warehouse: ROLLBACK
[0m18:06:59.860357 [debug] [ThreadPool]: On list_dbt_warehouse: Close
[0m18:06:59.861363 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_snapshots"
[0m18:06:59.864073 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:06:59.864367 [debug] [ThreadPool]: On list_dbt_snapshots: BEGIN
[0m18:06:59.864595 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:06:59.875183 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m18:06:59.875657 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:06:59.876041 [debug] [ThreadPool]: On list_dbt_snapshots: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_snapshots"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'snapshots'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'snapshots'
  
[0m18:06:59.881731 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.01 seconds
[0m18:06:59.883241 [debug] [ThreadPool]: On list_dbt_snapshots: ROLLBACK
[0m18:06:59.886138 [debug] [ThreadPool]: On list_dbt_snapshots: Close
[0m18:06:59.896173 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:59.896448 [debug] [MainThread]: On master: BEGIN
[0m18:06:59.896633 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:06:59.906100 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:06:59.906390 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:59.906608 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m18:06:59.917847 [debug] [MainThread]: SQL status: SELECT 0 in 0.01 seconds
[0m18:06:59.919638 [debug] [MainThread]: On master: ROLLBACK
[0m18:06:59.921347 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:59.921570 [debug] [MainThread]: On master: BEGIN
[0m18:06:59.924645 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m18:06:59.925064 [debug] [MainThread]: On master: COMMIT
[0m18:06:59.925325 [debug] [MainThread]: Using postgres connection "master"
[0m18:06:59.925551 [debug] [MainThread]: On master: COMMIT
[0m18:06:59.927589 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m18:06:59.928012 [debug] [MainThread]: On master: Close
[0m18:06:59.928988 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:06:59.929716 [info ] [MainThread]: 
[0m18:06:59.985416 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:06:59.985855 [info ] [Thread-1  ]: 1 of 6 START view model warehouse.stg_eltool__customers ........................ [RUN]
[0m18:06:59.986443 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:06:59.986688 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:06:59.986930 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.stg_eltool__customers
[0m18:06:59.990155 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:06:59.991022 [debug] [Thread-1  ]: finished collecting timing info
[0m18:06:59.991215 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:07:00.040597 [debug] [Thread-1  ]: Writing runtime SQL for node "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:07:00.041473 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:07:00.041652 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: BEGIN
[0m18:07:00.041801 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:07:00.051519 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:07:00.051918 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:07:00.052164 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__customers"} */

  create view "dbt"."warehouse"."stg_eltool__customers__dbt_tmp" as (
    with source as (
    select *
    from "dbt"."snapshots"."customers_snapshot"
), renamed as (
    select customer_id,
        zipcode,
        city,
        state_code,
        datetime_created::TIMESTAMP AS datetime_created,
        datetime_updated::TIMESTAMP AS datetime_updated,
        dbt_valid_from,
        dbt_valid_to
    from source
)
select *
from renamed
  );
[0m18:07:00.060409 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m18:07:00.067360 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:07:00.067621 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__customers"} */
alter table "dbt"."warehouse"."stg_eltool__customers__dbt_tmp" rename to "stg_eltool__customers"
[0m18:07:00.071738 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m18:07:00.091325 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: COMMIT
[0m18:07:00.091619 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:07:00.091790 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: COMMIT
[0m18:07:00.093980 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.100110 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:07:00.100343 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__customers"} */
drop view if exists "dbt"."warehouse"."stg_eltool__customers__dbt_backup" cascade
[0m18:07:00.103231 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m18:07:00.104706 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.105045 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__customers: Close
[0m18:07:00.105864 [info ] [Thread-1  ]: 1 of 6 OK created view model warehouse.stg_eltool__customers ................... [[32mCREATE VIEW[0m in 0.12s]
[0m18:07:00.106910 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:07:00.107217 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:07:00.107613 [info ] [Thread-1  ]: 2 of 6 START view model warehouse.stg_eltool__orders ........................... [RUN]
[0m18:07:00.108266 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.108562 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:07:00.108839 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.stg_eltool__orders
[0m18:07:00.111974 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.112581 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.112803 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:07:00.116904 [debug] [Thread-1  ]: Writing runtime SQL for node "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.117407 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.117589 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: BEGIN
[0m18:07:00.117748 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:07:00.131697 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:07:00.132572 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.132895 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__orders"} */

  create view "dbt"."warehouse"."stg_eltool__orders__dbt_tmp" as (
    with source as (
    select *
    from "dbt"."warehouse"."orders"
),
renamed as (
    select order_id,
        cust_id AS customer_id,
        order_status,
        order_purchase_timestamp::TIMESTAMP,
        order_approved_at::TIMESTAMP,
        order_delivered_carrier_date::TIMESTAMP,
        order_delivered_customer_date::TIMESTAMP,
        order_estimated_delivery_date::TIMESTAMP
        from source
)
select *
from renamed
  );
[0m18:07:00.137511 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
[0m18:07:00.142126 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.142375 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__orders"} */
alter table "dbt"."warehouse"."stg_eltool__orders__dbt_tmp" rename to "stg_eltool__orders"
[0m18:07:00.145689 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m18:07:00.147839 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: COMMIT
[0m18:07:00.148062 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.148237 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: COMMIT
[0m18:07:00.151760 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.155134 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:07:00.155369 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__orders"} */
drop view if exists "dbt"."warehouse"."stg_eltool__orders__dbt_backup" cascade
[0m18:07:00.158723 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m18:07:00.160330 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.160885 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__orders: Close
[0m18:07:00.162078 [info ] [Thread-1  ]: 2 of 6 OK created view model warehouse.stg_eltool__orders ...................... [[32mCREATE VIEW[0m in 0.05s]
[0m18:07:00.162778 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:07:00.163049 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.stg_eltool__state
[0m18:07:00.163598 [info ] [Thread-1  ]: 3 of 6 START view model warehouse.stg_eltool__state ............................ [RUN]
[0m18:07:00.164078 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.164258 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.stg_eltool__state
[0m18:07:00.164426 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.stg_eltool__state
[0m18:07:00.167092 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.167609 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.167822 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.stg_eltool__state
[0m18:07:00.175123 [debug] [Thread-1  ]: Writing runtime SQL for node "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.175737 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.175995 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: BEGIN
[0m18:07:00.176256 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:07:00.212767 [debug] [Thread-1  ]: SQL status: BEGIN in 0.04 seconds
[0m18:07:00.213108 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.213421 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__state"} */

  create view "dbt"."warehouse"."stg_eltool__state__dbt_tmp" as (
    with source as (
    select *
    from "dbt"."warehouse"."state"
),
renamed as (
    select state_identifier::INT AS state_id,
        state_code::VARCHAR(2) AS state_code,
        st_name::VARCHAR(30) AS state_name
    from source
)
select *
from renamed
  );
[0m18:07:00.216903 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
[0m18:07:00.221087 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.221307 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__state"} */
alter table "dbt"."warehouse"."stg_eltool__state__dbt_tmp" rename to "stg_eltool__state"
[0m18:07:00.223086 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m18:07:00.225370 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: COMMIT
[0m18:07:00.225581 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.225741 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: COMMIT
[0m18:07:00.228275 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.231434 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:07:00.231693 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.stg_eltool__state"} */
drop view if exists "dbt"."warehouse"."stg_eltool__state__dbt_backup" cascade
[0m18:07:00.233530 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m18:07:00.235186 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.235470 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.stg_eltool__state: Close
[0m18:07:00.236195 [info ] [Thread-1  ]: 3 of 6 OK created view model warehouse.stg_eltool__state ....................... [[32mCREATE VIEW[0m in 0.07s]
[0m18:07:00.236854 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.stg_eltool__state
[0m18:07:00.237118 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.fct_orders
[0m18:07:00.237992 [info ] [Thread-1  ]: 4 of 6 START table model warehouse.fct_orders .................................. [RUN]
[0m18:07:00.239010 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.239534 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.fct_orders
[0m18:07:00.240092 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.fct_orders
[0m18:07:00.244064 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.245008 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.245382 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.fct_orders
[0m18:07:00.271531 [debug] [Thread-1  ]: Writing runtime SQL for node "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.272920 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.273365 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: BEGIN
[0m18:07:00.273650 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:07:00.285844 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:07:00.286178 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.286442 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.fct_orders"} */


  create  table "dbt"."warehouse"."fct_orders__dbt_tmp"
  as (
    with orders as (
    select *
    from "dbt"."warehouse"."stg_eltool__orders"
)
select * from orders
  );
[0m18:07:00.296562 [debug] [Thread-1  ]: SQL status: SELECT 999 in 0.01 seconds
[0m18:07:00.299492 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.299688 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.fct_orders"} */
alter table "dbt"."warehouse"."fct_orders__dbt_tmp" rename to "fct_orders"
[0m18:07:00.301239 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m18:07:00.308044 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: COMMIT
[0m18:07:00.308343 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.308539 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: COMMIT
[0m18:07:00.311265 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.314008 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:07:00.314261 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.fct_orders"} */
drop table if exists "dbt"."warehouse"."fct_orders__dbt_backup" cascade
[0m18:07:00.315658 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m18:07:00.316886 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.317100 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.fct_orders: Close
[0m18:07:00.317629 [info ] [Thread-1  ]: 4 of 6 OK created table model warehouse.fct_orders ............................. [[32mSELECT 999[0m in 0.08s]
[0m18:07:00.318109 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.fct_orders
[0m18:07:00.318327 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.dim_customers
[0m18:07:00.318676 [info ] [Thread-1  ]: 5 of 6 START table model warehouse.dim_customers ............................... [RUN]
[0m18:07:00.319135 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.319312 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.dim_customers
[0m18:07:00.319476 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.dim_customers
[0m18:07:00.323523 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.324454 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.324707 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.dim_customers
[0m18:07:00.332023 [debug] [Thread-1  ]: Writing runtime SQL for node "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.332764 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.332972 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: BEGIN
[0m18:07:00.333158 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:07:00.345258 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:07:00.345861 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.346332 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.dim_customers"} */


  create  table "dbt"."warehouse"."dim_customers__dbt_tmp"
  as (
    with customers as (
    select *
    from "dbt"."warehouse"."stg_eltool__customers"
),
state as (
    select *
    from "dbt"."warehouse"."stg_eltool__state"
)
select c.customer_id,
    c.zipcode,
    c.city,
    c.state_code,
    s.state_name,
    c.datetime_created,
    c.datetime_updated,
    c.dbt_valid_from::TIMESTAMP as valid_from,
    CASE
        WHEN c.dbt_valid_to IS NULL THEN '9999-12-31'::TIMESTAMP
        ELSE c.dbt_valid_to::TIMESTAMP
    END as valid_to
from customers c
    join state s on c.state_code = s.state_code
  );
[0m18:07:00.354525 [debug] [Thread-1  ]: SQL status: SELECT 105 in 0.01 seconds
[0m18:07:00.358261 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.358480 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.dim_customers"} */
alter table "dbt"."warehouse"."dim_customers__dbt_tmp" rename to "dim_customers"
[0m18:07:00.359788 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m18:07:00.361972 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: COMMIT
[0m18:07:00.362386 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.362602 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: COMMIT
[0m18:07:00.364955 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.366949 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:07:00.367137 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.dim_customers"} */
drop table if exists "dbt"."warehouse"."dim_customers__dbt_backup" cascade
[0m18:07:00.368216 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m18:07:00.369344 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.369546 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.dim_customers: Close
[0m18:07:00.370118 [info ] [Thread-1  ]: 5 of 6 OK created table model warehouse.dim_customers .......................... [[32mSELECT 105[0m in 0.05s]
[0m18:07:00.370686 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.dim_customers
[0m18:07:00.371552 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.customer_orders
[0m18:07:00.372297 [info ] [Thread-1  ]: 6 of 6 START view model warehouse.customer_orders .............................. [RUN]
[0m18:07:00.373192 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.373400 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.customer_orders
[0m18:07:00.373605 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.customer_orders
[0m18:07:00.378947 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.379689 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.380055 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.customer_orders
[0m18:07:00.384419 [debug] [Thread-1  ]: Writing runtime SQL for node "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.384988 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.385162 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: BEGIN
[0m18:07:00.385317 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:07:00.397264 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:07:00.397575 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.397760 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.customer_orders"} */

  create view "dbt"."warehouse"."customer_orders__dbt_tmp" as (
    with orders as (
    select *
    from "dbt"."warehouse"."fct_orders"
),
customers as (
    select *
    from "dbt"."warehouse"."dim_customers"
)
select o.order_id,
    o.customer_id,
    o.order_status,
    o.order_purchase_timestamp,
    o.order_approved_at,
    o.order_delivered_carrier_date,
    o.order_delivered_customer_date,
    o.order_estimated_delivery_date,
    c.zipcode as customer_zipcode,
    c.city as customer_city,
    c.state_code as customer_state_code,
    c.state_name as customer_state_name
from orders o
    join customers c on o.customer_id = c.customer_id
    and o.order_purchase_timestamp >= c.valid_from
    and o.order_purchase_timestamp <= c.valid_to
  );
[0m18:07:00.401230 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
[0m18:07:00.403813 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.404019 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.customer_orders"} */
alter table "dbt"."warehouse"."customer_orders__dbt_tmp" rename to "customer_orders"
[0m18:07:00.406254 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m18:07:00.408355 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: COMMIT
[0m18:07:00.408587 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.408763 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: COMMIT
[0m18:07:00.410742 [debug] [Thread-1  ]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.413130 [debug] [Thread-1  ]: Using postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:07:00.413345 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "model.sde_dbt_tutorial.customer_orders"} */
drop view if exists "dbt"."warehouse"."customer_orders__dbt_backup" cascade
[0m18:07:00.414641 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m18:07:00.415794 [debug] [Thread-1  ]: finished collecting timing info
[0m18:07:00.416001 [debug] [Thread-1  ]: On model.sde_dbt_tutorial.customer_orders: Close
[0m18:07:00.416560 [info ] [Thread-1  ]: 6 of 6 OK created view model warehouse.customer_orders ......................... [[32mCREATE VIEW[0m in 0.04s]
[0m18:07:00.417067 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.customer_orders
[0m18:07:00.418147 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:07:00.418372 [debug] [MainThread]: Using postgres connection "master"
[0m18:07:00.418532 [debug] [MainThread]: On master: BEGIN
[0m18:07:00.418680 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:07:00.428936 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:07:00.429376 [debug] [MainThread]: On master: COMMIT
[0m18:07:00.429579 [debug] [MainThread]: Using postgres connection "master"
[0m18:07:00.429754 [debug] [MainThread]: On master: COMMIT
[0m18:07:00.430822 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m18:07:00.431177 [debug] [MainThread]: On master: Close
[0m18:07:00.431740 [info ] [MainThread]: 
[0m18:07:00.432205 [info ] [MainThread]: Finished running 4 view models, 2 table models in 0 hours 0 minutes and 0.65 seconds (0.65s).
[0m18:07:00.432552 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:07:00.432710 [debug] [MainThread]: Connection 'model.sde_dbt_tutorial.customer_orders' was properly closed.
[0m18:07:00.445855 [info ] [MainThread]: 
[0m18:07:00.446574 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:07:00.447210 [info ] [MainThread]: 
[0m18:07:00.447549 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6


============================== 2022-08-08 18:08:31.745976 | 36b64f55-82e3-40cc-9b65-4ec23cd074f4 ==============================
[0m18:08:31.746010 [info ] [MainThread]: Running with dbt=1.2.0
[0m18:08:31.747434 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m18:08:31.747677 [debug] [MainThread]: Tracking: do not track
[0m18:08:31.899526 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:08:31.900200 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:08:31.921557 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:08:31.924166 [info ] [MainThread]: 
[0m18:08:31.924853 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:08:31.926311 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_snapshots"
[0m18:08:31.936973 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:08:31.937378 [debug] [ThreadPool]: On list_dbt_snapshots: BEGIN
[0m18:08:31.937577 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:08:31.979383 [debug] [ThreadPool]: SQL status: BEGIN in 0.04 seconds
[0m18:08:31.979697 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:08:31.979888 [debug] [ThreadPool]: On list_dbt_snapshots: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_snapshots"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'snapshots'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'snapshots'
  
[0m18:08:31.983656 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:31.986159 [debug] [ThreadPool]: On list_dbt_snapshots: ROLLBACK
[0m18:08:31.987704 [debug] [ThreadPool]: On list_dbt_snapshots: Close
[0m18:08:31.988345 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_warehouse"
[0m18:08:31.991048 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:08:31.991391 [debug] [ThreadPool]: On list_dbt_warehouse: BEGIN
[0m18:08:31.991570 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:08:32.000279 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.000585 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:08:32.000845 [debug] [ThreadPool]: On list_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_warehouse"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m18:08:32.006897 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.01 seconds
[0m18:08:32.008296 [debug] [ThreadPool]: On list_dbt_warehouse: ROLLBACK
[0m18:08:32.009499 [debug] [ThreadPool]: On list_dbt_warehouse: Close
[0m18:08:32.014434 [debug] [MainThread]: Using postgres connection "master"
[0m18:08:32.014651 [debug] [MainThread]: On master: BEGIN
[0m18:08:32.014812 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:08:32.024681 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.024939 [debug] [MainThread]: Using postgres connection "master"
[0m18:08:32.025132 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m18:08:32.048009 [debug] [MainThread]: SQL status: SELECT 5 in 0.02 seconds
[0m18:08:32.049372 [debug] [MainThread]: On master: ROLLBACK
[0m18:08:32.050761 [debug] [MainThread]: Using postgres connection "master"
[0m18:08:32.050963 [debug] [MainThread]: On master: BEGIN
[0m18:08:32.053536 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m18:08:32.053817 [debug] [MainThread]: On master: COMMIT
[0m18:08:32.053997 [debug] [MainThread]: Using postgres connection "master"
[0m18:08:32.054154 [debug] [MainThread]: On master: COMMIT
[0m18:08:32.055356 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m18:08:32.055633 [debug] [MainThread]: On master: Close
[0m18:08:32.056147 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:08:32.056561 [info ] [MainThread]: 
[0m18:08:32.062960 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:08:32.063202 [info ] [Thread-1  ]: 1 of 10 START test accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable  [RUN]
[0m18:08:32.063718 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:08:32.063898 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:08:32.064072 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:08:32.079986 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:08:32.080815 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.081059 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:08:32.100594 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:08:32.104871 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:08:32.105217 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3: BEGIN
[0m18:08:32.105547 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.118194 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.118485 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:08:32.118770 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        order_status as value_field,
        count(*) as n_records

    from "dbt"."warehouse"."customer_orders"
    group by order_status

)

select *
from all_values
where value_field not in (
    'delivered','invoiced','shipped','processing','canceled','unavailable'
)



      
    ) dbt_internal_test
[0m18:08:32.125254 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.01 seconds
[0m18:08:32.127660 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.127863 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3: ROLLBACK
[0m18:08:32.129032 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3: Close
[0m18:08:32.129989 [info ] [Thread-1  ]: 1 of 10 PASS accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable  [[32mPASS[0m in 0.07s]
[0m18:08:32.131057 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:08:32.131437 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:08:32.131782 [info ] [Thread-1  ]: 2 of 10 START test assert_customer_dimension_has_no_row_loss ................... [RUN]
[0m18:08:32.132471 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:08:32.132659 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:08:32.132916 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:08:32.140659 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:08:32.141297 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.141512 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:08:32.143753 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:08:32.144380 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:08:32.144667 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss: BEGIN
[0m18:08:32.144834 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.153910 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.154227 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:08:32.154498 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      -- dim_customers must have the same number of rows as its staging counterpart
-- Therefore return records where this isn't true to make the test fail
select *
from (
        select dim_cust.customer_id
        from "dbt"."warehouse"."dim_customers" dim_cust
            left join "dbt"."warehouse"."stg_eltool__customers" stg_cust on dim_cust.customer_id = stg_cust.customer_id
        where stg_cust.customer_id is null
        UNION ALL
        select stg_cust.customer_id
        from "dbt"."warehouse"."stg_eltool__customers" stg_cust
            left join "dbt"."warehouse"."dim_customers" dim_cust on stg_cust.customer_id = dim_cust.customer_id
        where dim_cust.customer_id is null
    ) tmp
      
    ) dbt_internal_test
[0m18:08:32.157695 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.159457 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.159687 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss: ROLLBACK
[0m18:08:32.160928 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss: Close
[0m18:08:32.161379 [info ] [Thread-1  ]: 2 of 10 PASS assert_customer_dimension_has_no_row_loss ......................... [[32mPASS[0m in 0.03s]
[0m18:08:32.161860 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:08:32.162075 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:08:32.162312 [info ] [Thread-1  ]: 3 of 10 START test not_null_customer_orders_customer_id ........................ [RUN]
[0m18:08:32.162762 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:08:32.162937 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:08:32.163095 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:08:32.171751 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:08:32.172244 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.172444 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:08:32.174993 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:08:32.175503 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:08:32.175676 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0: BEGIN
[0m18:08:32.175825 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.184150 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.184447 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:08:32.184695 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from "dbt"."warehouse"."customer_orders"
where customer_id is null



      
    ) dbt_internal_test
[0m18:08:32.188001 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.189662 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.190137 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0: ROLLBACK
[0m18:08:32.191382 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0: Close
[0m18:08:32.191850 [info ] [Thread-1  ]: 3 of 10 PASS not_null_customer_orders_customer_id .............................. [[32mPASS[0m in 0.03s]
[0m18:08:32.192337 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:08:32.192551 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:08:32.192806 [info ] [Thread-1  ]: 4 of 10 START test not_null_dim_customers_customer_id .......................... [RUN]
[0m18:08:32.193253 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:08:32.193424 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:08:32.193585 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:08:32.197404 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:08:32.197950 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.198170 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:08:32.200582 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:08:32.201491 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:08:32.201706 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d: BEGIN
[0m18:08:32.201955 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.213143 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.213660 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:08:32.214215 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from "dbt"."warehouse"."dim_customers"
where customer_id is null



      
    ) dbt_internal_test
[0m18:08:32.217833 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.219561 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.219807 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d: ROLLBACK
[0m18:08:32.221111 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d: Close
[0m18:08:32.221766 [info ] [Thread-1  ]: 4 of 10 PASS not_null_dim_customers_customer_id ................................ [[32mPASS[0m in 0.03s]
[0m18:08:32.222321 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:08:32.222649 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:08:32.222919 [info ] [Thread-1  ]: 5 of 10 START test not_null_stg_eltool__customers_customer_id .................. [RUN]
[0m18:08:32.223372 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:08:32.223548 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:08:32.223713 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:08:32.228844 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:08:32.240192 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.240853 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:08:32.244824 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:08:32.245747 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:08:32.246019 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df: BEGIN
[0m18:08:32.246253 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.271701 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m18:08:32.272074 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:08:32.272476 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from "dbt"."warehouse"."stg_eltool__customers"
where customer_id is null



      
    ) dbt_internal_test
[0m18:08:32.275007 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.276505 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.276719 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df: ROLLBACK
[0m18:08:32.277590 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df: Close
[0m18:08:32.278294 [info ] [Thread-1  ]: 5 of 10 PASS not_null_stg_eltool__customers_customer_id ........................ [[32mPASS[0m in 0.06s]
[0m18:08:32.278854 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:08:32.279169 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:08:32.279560 [info ] [Thread-1  ]: 6 of 10 START test source_not_null_warehouse_customers_customer_id ............. [RUN]
[0m18:08:32.280443 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:08:32.280747 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:08:32.281040 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:08:32.286688 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:08:32.287436 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.287761 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:08:32.289939 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:08:32.290960 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:08:32.291282 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6: BEGIN
[0m18:08:32.291940 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.304157 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.304558 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:08:32.304900 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from "dbt"."warehouse"."customers"
where customer_id is null



      
    ) dbt_internal_test
[0m18:08:32.308612 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.310976 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.311294 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6: ROLLBACK
[0m18:08:32.312896 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6: Close
[0m18:08:32.313596 [info ] [Thread-1  ]: 6 of 10 PASS source_not_null_warehouse_customers_customer_id ................... [[32mPASS[0m in 0.03s]
[0m18:08:32.314450 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:08:32.314859 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:08:32.315325 [info ] [Thread-1  ]: 7 of 10 START test source_not_null_warehouse_orders_order_id ................... [RUN]
[0m18:08:32.316159 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:08:32.316621 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:08:32.317100 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:08:32.325772 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:08:32.326421 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.326629 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:08:32.328685 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:08:32.329283 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:08:32.329502 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76: BEGIN
[0m18:08:32.329686 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.339929 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.340277 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:08:32.340642 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from "dbt"."warehouse"."orders"
where order_id is null



      
    ) dbt_internal_test
[0m18:08:32.343558 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.344921 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.345128 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76: ROLLBACK
[0m18:08:32.346272 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76: Close
[0m18:08:32.346698 [info ] [Thread-1  ]: 7 of 10 PASS source_not_null_warehouse_orders_order_id ......................... [[32mPASS[0m in 0.03s]
[0m18:08:32.347106 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:08:32.347319 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:08:32.347594 [info ] [Thread-1  ]: 8 of 10 START test source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_  [RUN]
[0m18:08:32.348102 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:08:32.348286 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:08:32.348454 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:08:32.360107 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:08:32.360622 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.360820 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:08:32.362873 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:08:32.363339 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:08:32.363503 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2: BEGIN
[0m18:08:32.363654 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.375645 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.375961 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:08:32.376148 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with child as (
    select cust_id as from_field
    from "dbt"."warehouse"."orders"
    where cust_id is not null
),

parent as (
    select customer_id as to_field
    from "dbt"."warehouse"."customers"
)

select
    from_field

from child
left join parent
    on child.from_field = parent.to_field

where parent.to_field is null



      
    ) dbt_internal_test
[0m18:08:32.379519 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.380932 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.381154 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2: ROLLBACK
[0m18:08:32.382935 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2: Close
[0m18:08:32.383438 [info ] [Thread-1  ]: 8 of 10 PASS source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_  [[32mPASS[0m in 0.04s]
[0m18:08:32.384252 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:08:32.384532 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:08:32.384879 [info ] [Thread-1  ]: 9 of 10 START test source_unique_warehouse_orders_order_id ..................... [RUN]
[0m18:08:32.385393 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:08:32.385591 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:08:32.385755 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:08:32.393583 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:08:32.394108 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.394321 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:08:32.396413 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:08:32.396865 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:08:32.397028 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f: BEGIN
[0m18:08:32.397172 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.407723 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.408316 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:08:32.408734 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from "dbt"."warehouse"."orders"
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:08:32.428439 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.02 seconds
[0m18:08:32.430835 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.431083 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f: ROLLBACK
[0m18:08:32.436662 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f: Close
[0m18:08:32.437514 [info ] [Thread-1  ]: 9 of 10 PASS source_unique_warehouse_orders_order_id ........................... [[32mPASS[0m in 0.05s]
[0m18:08:32.438602 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:08:32.439113 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:08:32.439472 [info ] [Thread-1  ]: 10 of 10 START test unique_customer_orders_order_id ............................ [RUN]
[0m18:08:32.440467 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:08:32.440805 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:08:32.441001 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:08:32.445515 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:08:32.445989 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.446171 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:08:32.448154 [debug] [Thread-1  ]: Writing runtime SQL for node "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:08:32.448590 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:08:32.448750 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc: BEGIN
[0m18:08:32.448895 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m18:08:32.458736 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.459002 [debug] [Thread-1  ]: Using postgres connection "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:08:32.459179 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "node_id": "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    order_id as unique_field,
    count(*) as n_records

from "dbt"."warehouse"."customer_orders"
where order_id is not null
group by order_id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:08:32.463117 [debug] [Thread-1  ]: SQL status: SELECT 1 in 0.0 seconds
[0m18:08:32.464652 [debug] [Thread-1  ]: finished collecting timing info
[0m18:08:32.464895 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc: ROLLBACK
[0m18:08:32.466087 [debug] [Thread-1  ]: On test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc: Close
[0m18:08:32.466552 [info ] [Thread-1  ]: 10 of 10 PASS unique_customer_orders_order_id .................................. [[32mPASS[0m in 0.03s]
[0m18:08:32.467127 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:08:32.468389 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:08:32.468628 [debug] [MainThread]: Using postgres connection "master"
[0m18:08:32.468804 [debug] [MainThread]: On master: BEGIN
[0m18:08:32.469054 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m18:08:32.479774 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:08:32.480040 [debug] [MainThread]: On master: COMMIT
[0m18:08:32.480236 [debug] [MainThread]: Using postgres connection "master"
[0m18:08:32.480401 [debug] [MainThread]: On master: COMMIT
[0m18:08:32.481505 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m18:08:32.481715 [debug] [MainThread]: On master: Close
[0m18:08:32.482110 [info ] [MainThread]: 
[0m18:08:32.482388 [info ] [MainThread]: Finished running 10 tests in 0 hours 0 minutes and 0.56 seconds (0.56s).
[0m18:08:32.482660 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:08:32.482841 [debug] [MainThread]: Connection 'test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc' was properly closed.
[0m18:08:32.496426 [info ] [MainThread]: 
[0m18:08:32.496765 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:08:32.497367 [info ] [MainThread]: 
[0m18:08:32.497645 [info ] [MainThread]: Done. PASS=10 WARN=0 ERROR=0 SKIP=0 TOTAL=10


============================== 2022-08-08 18:09:54.096103 | 7b50cbcf-6140-4385-be2e-6c02e27a6c66 ==============================
[0m18:09:54.096148 [info ] [MainThread]: Running with dbt=1.2.0
[0m18:09:54.097794 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m18:09:54.098033 [debug] [MainThread]: Tracking: do not track
[0m18:09:54.238839 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:09:54.239533 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:09:54.260668 [info ] [MainThread]: Found 6 models, 10 tests, 1 snapshot, 0 analyses, 256 macros, 0 operations, 0 seed files, 3 sources, 0 exposures, 0 metrics
[0m18:09:54.263138 [info ] [MainThread]: 
[0m18:09:54.263690 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m18:09:54.264830 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_warehouse"
[0m18:09:54.276019 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:09:54.276313 [debug] [ThreadPool]: On list_dbt_warehouse: BEGIN
[0m18:09:54.276504 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:09:54.306452 [debug] [ThreadPool]: SQL status: BEGIN in 0.03 seconds
[0m18:09:54.306782 [debug] [ThreadPool]: Using postgres connection "list_dbt_warehouse"
[0m18:09:54.306947 [debug] [ThreadPool]: On list_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_warehouse"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m18:09:54.310684 [debug] [ThreadPool]: SQL status: SELECT 9 in 0.0 seconds
[0m18:09:54.313130 [debug] [ThreadPool]: On list_dbt_warehouse: ROLLBACK
[0m18:09:54.314266 [debug] [ThreadPool]: On list_dbt_warehouse: Close
[0m18:09:54.314864 [debug] [ThreadPool]: Acquiring new postgres connection "list_dbt_snapshots"
[0m18:09:54.317351 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:09:54.317676 [debug] [ThreadPool]: On list_dbt_snapshots: BEGIN
[0m18:09:54.317922 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m18:09:54.326244 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m18:09:54.326598 [debug] [ThreadPool]: Using postgres connection "list_dbt_snapshots"
[0m18:09:54.326818 [debug] [ThreadPool]: On list_dbt_snapshots: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "list_dbt_snapshots"} */
select
      'dbt' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'snapshots'
    union all
    select
      'dbt' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'snapshots'
  
[0m18:09:54.331503 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.0 seconds
[0m18:09:54.332928 [debug] [ThreadPool]: On list_dbt_snapshots: ROLLBACK
[0m18:09:54.333940 [debug] [ThreadPool]: On list_dbt_snapshots: Close
[0m18:09:54.339331 [debug] [MainThread]: Using postgres connection "master"
[0m18:09:54.339579 [debug] [MainThread]: On master: BEGIN
[0m18:09:54.339744 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:09:54.347448 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m18:09:54.347770 [debug] [MainThread]: Using postgres connection "master"
[0m18:09:54.347994 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m18:09:54.364386 [debug] [MainThread]: SQL status: SELECT 5 in 0.02 seconds
[0m18:09:54.366028 [debug] [MainThread]: On master: ROLLBACK
[0m18:09:54.367448 [debug] [MainThread]: On master: Close
[0m18:09:54.368195 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:09:54.368611 [info ] [MainThread]: 
[0m18:09:54.374964 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:09:54.375461 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:09:54.375672 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:09:54.375869 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.stg_eltool__orders
[0m18:09:54.379186 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.stg_eltool__orders"
[0m18:09:54.381098 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.381327 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:09:54.381520 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.382056 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.stg_eltool__orders
[0m18:09:54.382263 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.stg_eltool__state
[0m18:09:54.382769 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:09:54.382971 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.stg_eltool__state
[0m18:09:54.383134 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.stg_eltool__state
[0m18:09:54.385720 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.stg_eltool__state"
[0m18:09:54.387260 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.387498 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.stg_eltool__state
[0m18:09:54.387759 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.388285 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.stg_eltool__state
[0m18:09:54.388512 [debug] [Thread-1  ]: Began running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:09:54.388946 [debug] [Thread-1  ]: Acquiring new postgres connection "snapshot.sde_dbt_tutorial.customers_snapshot"
[0m18:09:54.389128 [debug] [Thread-1  ]: Began compiling node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:09:54.389296 [debug] [Thread-1  ]: Compiling snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:09:54.392867 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.393087 [debug] [Thread-1  ]: Began executing node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:09:54.393268 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.393699 [debug] [Thread-1  ]: Finished running node snapshot.sde_dbt_tutorial.customers_snapshot
[0m18:09:54.393909 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:09:54.394403 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:09:54.394618 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:09:54.394807 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:09:54.410759 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6"
[0m18:09:54.413614 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.413891 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:09:54.414133 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.414926 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_not_null_warehouse_customers_customer_id.ae7452c4c6
[0m18:09:54.415241 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:09:54.415689 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:09:54.415998 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:09:54.416208 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:09:54.421985 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76"
[0m18:09:54.423203 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.423428 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:09:54.423611 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.424037 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_not_null_warehouse_orders_order_id.c85a1f0b76
[0m18:09:54.424299 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:09:54.424792 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:09:54.425041 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:09:54.425229 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:09:54.434465 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2"
[0m18:09:54.436075 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.436326 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:09:54.436509 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.436970 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_relationships_warehouse_orders_cust_id__customer_id__source_warehouse_customers_.d3f9188fc2
[0m18:09:54.437185 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:09:54.437582 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:09:54.437756 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:09:54.437938 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:09:54.444722 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f"
[0m18:09:54.446231 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.446455 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:09:54.446644 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.447141 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.source_unique_warehouse_orders_order_id.839fb43d0f
[0m18:09:54.447361 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.fct_orders
[0m18:09:54.447820 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.fct_orders"
[0m18:09:54.447997 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.fct_orders
[0m18:09:54.448162 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.fct_orders
[0m18:09:54.450809 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.fct_orders"
[0m18:09:54.452380 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.452635 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.fct_orders
[0m18:09:54.452834 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.453326 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.fct_orders
[0m18:09:54.453538 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:09:54.454005 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:09:54.454178 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:09:54.454338 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.stg_eltool__customers
[0m18:09:54.457116 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.stg_eltool__customers"
[0m18:09:54.458648 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.458880 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:09:54.459066 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.459578 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.stg_eltool__customers
[0m18:09:54.460354 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.dim_customers
[0m18:09:54.460862 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.dim_customers"
[0m18:09:54.461059 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.dim_customers
[0m18:09:54.461229 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.dim_customers
[0m18:09:54.464347 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.dim_customers"
[0m18:09:54.465688 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.465894 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.dim_customers
[0m18:09:54.466076 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.466559 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.dim_customers
[0m18:09:54.466775 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:09:54.467376 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:09:54.467617 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:09:54.467815 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:09:54.472087 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df"
[0m18:09:54.473617 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.473820 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:09:54.474000 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.474452 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.not_null_stg_eltool__customers_customer_id.4bd58324df
[0m18:09:54.474663 [debug] [Thread-1  ]: Began running node model.sde_dbt_tutorial.customer_orders
[0m18:09:54.475092 [debug] [Thread-1  ]: Acquiring new postgres connection "model.sde_dbt_tutorial.customer_orders"
[0m18:09:54.475268 [debug] [Thread-1  ]: Began compiling node model.sde_dbt_tutorial.customer_orders
[0m18:09:54.475428 [debug] [Thread-1  ]: Compiling model.sde_dbt_tutorial.customer_orders
[0m18:09:54.478775 [debug] [Thread-1  ]: Writing injected SQL for node "model.sde_dbt_tutorial.customer_orders"
[0m18:09:54.480533 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.480801 [debug] [Thread-1  ]: Began executing node model.sde_dbt_tutorial.customer_orders
[0m18:09:54.481023 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.481632 [debug] [Thread-1  ]: Finished running node model.sde_dbt_tutorial.customer_orders
[0m18:09:54.481877 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:09:54.482495 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:09:54.482730 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:09:54.482894 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:09:54.488517 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss"
[0m18:09:54.490371 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.490650 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:09:54.490846 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.491397 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.assert_customer_dimension_has_no_row_loss
[0m18:09:54.491624 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:09:54.492114 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:09:54.492291 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:09:54.492457 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:09:54.496756 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d"
[0m18:09:54.498229 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.498429 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:09:54.498603 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.499079 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.not_null_dim_customers_customer_id.dd91cd1c8d
[0m18:09:54.499284 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:09:54.499732 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:09:54.499908 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:09:54.500070 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:09:54.511912 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3"
[0m18:09:54.513798 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.514069 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:09:54.514261 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.514830 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.accepted_values_customer_orders_order_status__delivered__invoiced__shipped__processing__canceled__unavailable.55819769c3
[0m18:09:54.515070 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:09:54.515518 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:09:54.515728 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:09:54.515892 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:09:54.520981 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0"
[0m18:09:54.522583 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.522853 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:09:54.523074 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.523639 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.not_null_customer_orders_customer_id.582e6bcaa0
[0m18:09:54.523883 [debug] [Thread-1  ]: Began running node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:09:54.524457 [debug] [Thread-1  ]: Acquiring new postgres connection "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:09:54.524670 [debug] [Thread-1  ]: Began compiling node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:09:54.524860 [debug] [Thread-1  ]: Compiling test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:09:54.529899 [debug] [Thread-1  ]: Writing injected SQL for node "test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc"
[0m18:09:54.530961 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.531207 [debug] [Thread-1  ]: Began executing node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:09:54.531412 [debug] [Thread-1  ]: finished collecting timing info
[0m18:09:54.531868 [debug] [Thread-1  ]: Finished running node test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc
[0m18:09:54.532990 [debug] [MainThread]: Connection 'master' was properly closed.
[0m18:09:54.533200 [debug] [MainThread]: Connection 'test.sde_dbt_tutorial.unique_customer_orders_order_id.352340f7cc' was properly closed.
[0m18:09:54.543497 [info ] [MainThread]: Done.
[0m18:09:54.548952 [debug] [MainThread]: Acquiring new postgres connection "generate_catalog"
[0m18:09:54.549182 [info ] [MainThread]: Building catalog
[0m18:09:54.552746 [debug] [ThreadPool]: Acquiring new postgres connection "dbt.information_schema"
[0m18:09:54.560961 [debug] [ThreadPool]: Using postgres connection "dbt.information_schema"
[0m18:09:54.561239 [debug] [ThreadPool]: On dbt.information_schema: BEGIN
[0m18:09:54.561442 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:09:54.571754 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m18:09:54.572120 [debug] [ThreadPool]: Using postgres connection "dbt.information_schema"
[0m18:09:54.572308 [debug] [ThreadPool]: On dbt.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "sde_dbt_tutorial", "target_name": "dev", "connection_name": "dbt.information_schema"} */

    
    

    select
        'dbt' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        tbl_desc.description as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        col_desc.description as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid
    left outer join pg_catalog.pg_description tbl_desc on (tbl_desc.objoid = tbl.oid and tbl_desc.objsubid = 0)
    left outer join pg_catalog.pg_description col_desc on (col_desc.objoid = tbl.oid and col_desc.objsubid = col.attnum)

    where (upper(sch.nspname) = upper('snapshots') or upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence in ('p', 'u') -- [p]ermanent table or [u]nlogged table. Exclude [t]emporary tables
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
[0m18:09:54.588996 [debug] [ThreadPool]: SQL status: SELECT 75 in 0.02 seconds
[0m18:09:54.604731 [debug] [ThreadPool]: On dbt.information_schema: ROLLBACK
[0m18:09:54.606520 [debug] [ThreadPool]: On dbt.information_schema: Close
[0m18:09:54.622858 [info ] [MainThread]: Catalog written to /Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project/sde_dbt_tutorial/target/catalog.json
[0m18:09:54.623859 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m18:09:54.624063 [debug] [MainThread]: Connection 'dbt.information_schema' was properly closed.


============================== 2022-08-08 18:10:23.626024 | cf8042c5-401e-4153-abc4-d3a7782634f6 ==============================
[0m18:10:23.626057 [info ] [MainThread]: Running with dbt=1.2.0
[0m18:10:23.627571 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/AlbertLok/Documents/Data Boot Camp/simple_dbt_project/simple_dbt_project', 'send_anonymous_usage_stats': False, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m18:10:23.627871 [debug] [MainThread]: Tracking: do not track
[0m18:10:23.650893 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m18:10:23.651993 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m18:10:23.652327 [info ] [MainThread]: 
[0m18:10:23.652563 [info ] [MainThread]: 
[0m18:10:23.652790 [info ] [MainThread]: Press Ctrl+C to exit.
